version: '3'
services:
  nginx:
    image: nginx:alpine
    ports:
      - "5000:80"
    volumes:
      - ./web-app/build:/usr/share/nginx/html
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - llm
      - tts
      - asr
  llm:
    image: ghcr.io/huggingface/text-generation-inference:1.1.0
    ports:
      - "8080:8080"
    environment:
      - MODEL_ID=TheBloke/openchat_3.5-AWQ
      - PORT=8080
      - QUANTIZE=awq
      - MAX_INPUT_LEN=3696
      - MAX_TOTAL_TOKENS=4096
      - MAX_BATCH_PREFILL_TOKENS=4096
      - CUDA_MEMORY_FRACTION=0.6
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  tts:
    image: ghcr.io/coqui-ai/xtts-streaming-server:main-cuda121-818a108b41be2dd43dada04bd319fdfcdabc5c6a
    ports:
      - "8000:80"
    volumes:
      - /media/julian/Workdisk/models/ai_voice_chat:/app/tts_models
    environment:
      - COQUI_TOS_AGREED=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  asr:
    image: onerahmet/openai-whisper-asr-webservice:v1.2.4-gpu
    ports: 
      - "9000:9000"
    environment:
      - ASR_ENGINE=faster_whisper
      - ASR_MODEL=large-v3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
